{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Proof of Concept with Small Dataset\n",
    "\n",
    "In this initial stage, I'll develop a functioning GPT-2 architecture from scratch and test it on a smaller text sample, namely the Lord of the Rings books. This approach allows it to be validated while working within reasonable computational constraints.\n",
    "\n",
    "**Objectives:**\n",
    "- Build all essential components of the transformer architecture\n",
    "- Train the model on a limited dataset to demonstrate operational functionality\n",
    "- Establish evaluation metrics for baseline performance\n",
    "- Document architectural decisions and implementation details\n",
    "\n",
    "While this initial model won't produce state-of-the-art text generation results, it will serve as a critical proof of concept, confirming that the implementation correctly reflects the theoretical foundations of transformer-based language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and sampling\n",
    "\n",
    "Below is a diagram showing all of the stages to design an LLM.  I will be using Bite Pair Encoding a Sherlock Holmes texts in the below code block to complete Stage 1 part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GPT Structure](resources/Stage_1_of_LLM_Design.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken\n",
    "%pip install tqdm\n",
    "from tqdm.auto import tqdm \n",
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the general config for the GPT model, this is exactly the same as used apart from the context_length which was 1024 is in the real training. Remember the context length in LLMs refers to the maximum number of tokens the model can consider as input when generating text, influencing its ability to maintain coherence and capture long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I create a window slider to tockenize the text into chunks for later processes, here is a graphical representation.  Note the stride is important because this governs whether there is overlap between tockens which would negatively effect trianing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![GPT Structure](resources/window.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for GPT training that converts text into overlapping token sequences.\n",
    "    \n",
    "    This class splits a tokenized text into chunks of specified length with a stride,\n",
    "    creating input-target pairs where the target is the input shifted by one position.\n",
    "    \"\"\"\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader(txt, batch_size=2, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for GPT training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")    \n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is turn the text into tokens, for modern LLMs we use Byte Pair encoding, this is a subword tokenizera nd results in 50257 unique possible ids.  These token embeddings are then moved into higher dimensional space (768 for GPT 2) to give the tokens meaning in relation to each other.  This is then combined with a positional embedding to make INPUT EMBEDDINGS.\n",
    "\n",
    "The graph below shows the attention mechanism which is central to the llm work flow, this is a very important part where we take token and give them meaning to turn them into context vectors.  They do this by calculating relationships between tokens using queries, keys, and values, then weighting and summing the values to create a context vector for each token, capturing the importance of other tokens in the sequence. This allows the model to understand the context and relationships within the input.\n",
    "\n",
    "Queries * Keys transpose gives an attention score.  This is then scaled by the square root of the keys dimension, drop out applied and soft max to get attention weights.  Finally attention weights are mutliplied by values matrices to give context vectors.  This is for one attention head, in the context of my code I have 12 heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GPT Structure](resources/multihead.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism as described in the 'Attention is All You Need' paper.\n",
    "    \n",
    "    This implementation includes causal (masked) self-attention to ensure that\n",
    "    predictions for a position can only depend on known outputs from previous positions.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, d_out,context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0)\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\",torch.triu(torch.ones(context_length, context_length),\n",
    "        diagonal=1))\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view( b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above there are two interesting processes as well.  Masked fill covers words after the current token because the transformer predicts the next word without seeing it.  If you imagine the matrix, this mask covers the top right diagonal half.  Also there is drop out where random elements are dropped to force lazy neurons to be active and not rely on others to do the lifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer normalization module that normalizes inputs across the feature dimension.\n",
    "    \n",
    "    This is a custom implementation of Layer Normalization as described in the paper\n",
    "    'Layer Normalization' by Jimmy Lei Ba et al.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian Error Linear Unit activation function module.\n",
    "    \n",
    "    This is an implementation of GELU as described in the paper\n",
    "    'Gaussian Error Linear Units (GELUs)' by Dan Hendrycks and Kevin Gimpel.\n",
    "    GELU provides a smooth, non-linear activation that performs well in\n",
    "    transformer-based language models.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network used in transformer blocks.\n",
    "    \n",
    "    This implements the FFN as described in 'Attention is All You Need',\n",
    "    consisting of two linear transformations with a GELU activation in between.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![GPT Structure](resources/transformer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphical representation of the steps in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer block as used in GPT architecture.\n",
    "    \n",
    "    Each block includes a multi-head self-attention layer followed by a feed-forward\n",
    "    network, with layer normalization and residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GPT model implementation based on the GPT-2 architecture.\n",
    "    \n",
    "    This model includes token and position embeddings, multiple transformer blocks,\n",
    "    and a final output head for token prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GPT Structure](resources/Stage_2_of_LLM_Design.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Hello I am 950ackersframeuffمブ pi…] materials polish\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Hello I am\"\n",
    "\n",
    "\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE FILE PATH BELOW WILL NEED TO BE CHANGED WHEN RUN LOCALLY OR ON COLAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"resources/holmes.txt\", \"r\", encoding=\"utf-8\") as h:\n",
    "    holmes_text = h.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(holmes_text))\n",
    "train_data = holmes_text[:split_idx]\n",
    "val_data = holmes_text[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def create_dataloader(data, batch_size, max_length, stride, drop_last, shuffle, num_workers,device):\n",
    "   \"\"\"\n",
    "   Create a DataLoader for train or validation data.\n",
    "   \"\"\"\n",
    "   dataset = GPTDataset(data, tokenizer, max_length, stride)\n",
    "   dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers,)\n",
    "   return dataloader\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    train_data,\n",
    "    batch_size=16,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "    val_data,\n",
    "    batch_size=16,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 65536\n",
      "Validation tokens: 7424\n",
      "All tokens: 72960\n"
     ]
    }
   ],
   "source": [
    "#this is just an information cell to show sizes and can be removed if desired\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate loss for a single batch\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"\n",
    "    Calculate cross-entropy loss for a single batch.\n",
    "    \n",
    "    This function handles potential memory errors by falling back to CPU if needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "        logits = model(input_batch)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), target_batch.reshape(-1))\n",
    "        return loss\n",
    "    except RuntimeError as e:\n",
    "        if \"MPS backend out of memory\" in str(e):\n",
    "            print(\"MPS memory error, falling back to CPU for this batch\")\n",
    "            model_cpu = model.to(\"cpu\")\n",
    "            input_batch_cpu, target_batch_cpu = input_batch.to(\"cpu\"), target_batch.to(\"cpu\")\n",
    "            logits = model_cpu(input_batch_cpu)\n",
    "            loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), target_batch_cpu.reshape(-1))\n",
    "            model.to(device)  \n",
    "            return loss.to(device)\n",
    "        else:\n",
    "            raise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, max_batches=None):\n",
    "    \"\"\"\n",
    "    Calculate average loss across multiple batches with progress bar.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    # Create a progress bar for loss calculation\n",
    "    max_batches = max_batches if max_batches is not None else len(data_loader)\n",
    "    pbar = tqdm(total=max_batches, desc=\"Calculating loss\", leave=False, position=2)\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "        # Process only max_batches if specified\n",
    "        if max_batches is not None and batch_idx >= max_batches:\n",
    "            break\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        loss = calc_loss_batch(inputs, targets, model, device)\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return total_loss / max(1, batch_count)  # Avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter, progress_callback=None):\n",
    "    \"\"\"Evaluate model on train and validation sets with progress tracking\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Evaluate on training set\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, max_batches=eval_iter)\n",
    "        if progress_callback:\n",
    "            progress_callback(1)  # Update progress by 1 step\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, max_batches=eval_iter)\n",
    "        if progress_callback:\n",
    "            progress_callback(1)  # Update progress by 1 step\n",
    "\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram of how the training loop works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GPT Structure](resources/train_process.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainable weights in this model fall into the token embeddings, positional embeddings, key matrix, value matrix, query matrix, scale and shift in the layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} started.\")\n",
    "\n",
    "        epoch_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "                             total=len(train_loader), position=0)\n",
    "\n",
    "        for batch_idx, (input_batch, target_batch) in enumerate(epoch_progress):\n",
    "            try:\n",
    "                input_batch = input_batch.to(device)\n",
    "                target_batch = target_batch.to(device)\n",
    "\n",
    "                optimizer.zero_grad() \n",
    "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "                loss.backward()  \n",
    "                optimizer.step()  \n",
    "                tokens_seen += input_batch.numel()\n",
    "                global_step += 1\n",
    "\n",
    "                epoch_progress.set_postfix({\n",
    "                    'loss': f\"{loss.item():.3f}\",\n",
    "                    'tokens': tokens_seen\n",
    "                })\n",
    "\n",
    "                if global_step % eval_freq == 0:\n",
    "                    print(f\"\\nEvaluating at step {global_step}...\")\n",
    "\n",
    "                    with tqdm(total=2, desc=\"Evaluation\", position=1) as eval_progress:\n",
    "                        train_loss, val_loss = evaluate_model(\n",
    "                            model, train_loader, val_loader, device, eval_iter,\n",
    "                            progress_callback=lambda x: eval_progress.update(x))\n",
    "\n",
    "                    train_losses.append(train_loss)\n",
    "                    val_losses.append(val_loss)\n",
    "                    track_tokens_seen.append(tokens_seen)\n",
    "                    print(f\"Epoch {epoch+1} (Step {global_step:06d}): \"\n",
    "                          f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"MPS\" in str(e):\n",
    "                    print(f\"MPS memory error at Epoch {epoch+1}, Batch {batch_idx+1}. Falling back to CPU.\")\n",
    "                    device_cpu = torch.device(\"cpu\")\n",
    "                    input_batch = input_batch.to(device_cpu)\n",
    "                    target_batch = target_batch.to(device_cpu)\n",
    "                    model.to(device_cpu)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = calc_loss_batch(input_batch, target_batch, model, device_cpu)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    tokens_seen += input_batch.numel()\n",
    "                    global_step += 1\n",
    "\n",
    "                    epoch_progress.set_postfix({\n",
    "                        'loss (CPU)': f\"{loss.item():.3f}\",\n",
    "                        'tokens': tokens_seen\n",
    "                    })\n",
    "\n",
    "                    model.to(device)\n",
    "                    input_batch = input_batch.to(device)\n",
    "                    target_batch = target_batch.to(device)\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "        epoch_progress.close()\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"EPOCH {epoch+1} COMPLETED - MODEL GENERATION:\")\n",
    "        \n",
    "        model.eval()\n",
    "        context_size = model.pos_emb.weight.shape[0]\n",
    "        encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "        with torch.no_grad():\n",
    "            token_ids = generate_text_simple(\n",
    "                model=model, idx=encoded,\n",
    "                max_new_tokens=50, context_size=context_size\n",
    "            )\n",
    "        \n",
    "        full_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        continuation = full_text[len(start_context):]\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=20, eval_iter=1,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    # Ensure model and idx are on the same device\n",
    "    device = next(model.parameters()).device\n",
    "    idx = idx.to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            neg_inf = torch.tensor(float(\"-inf\"), device=device)\n",
    "            logits = torch.where(logits < min_val, neg_inf, logits)\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if eos_id is not None and (idx_next == eos_id).all():  \n",
    "            break\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "model=model,\n",
    "idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "max_new_tokens=15,\n",
    "context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "top_k=25,\n",
    "temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
